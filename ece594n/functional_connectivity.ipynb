{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from func import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph signal processing is a new tool to model brain organization and function. The brain is composed of several Region of Interests(ROIs). Brain graphs provide an efficient way for modeling the human brain connectome, by associating nodes to the brain regions, and defining edges via anatomical or functional connections. These ROIs are connected to some regions of interests with the highest connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "haxby_dataset = datasets.fetch_haxby(subjects=[3], fetch_stimuli=True, data_dir=data_dir)\n",
    "func_file = haxby_dataset.func[0]\n",
    "\n",
    "# Standardizing\n",
    "mask_vt_file = haxby_dataset.mask_vt[0]\n",
    "masker = NiftiMasker(mask_img=mask_vt_file, standardize=True)\n",
    "\n",
    "# cognitive annotations\n",
    "import pandas as pd\n",
    "behavioral = pd.read_csv(haxby_dataset.session_target[0], delimiter=' ')\n",
    "X = masker.fit_transform(func_file)\n",
    "y = behavioral['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = y.unique()\n",
    "print(categories)\n",
    "print('y:', y.shape)\n",
    "print('X:', X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 1452 time points in the imaging data, and for each time point we have recordings of fMRI activity across 675 brain regions.\n",
    "\n",
    "to do:\n",
    "1. visualize connectome in 3D \n",
    "    - derive coordinates of POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "import nilearn.connectome\n",
    "\n",
    "# Estimating connectomes and save for pytorch to load\n",
    "corr_measure = nilearn.connectome.ConnectivityMeasure(kind=\"correlation\")\n",
    "conn = corr_measure.fit_transform([X])[0]\n",
    "\n",
    "n_regions_extracted = X.shape[-1]\n",
    "title = 'Correlation between %d regions' % n_regions_extracted\n",
    "\n",
    "print('Correlation matrix shape:',conn.shape)\n",
    "\n",
    "# First plot the matrix\n",
    "from nilearn import plotting\n",
    "display = plotting.plot_matrix(conn, vmax=1, vmin=-1,\n",
    "                               colorbar=True, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = make_group_graph([conn], self_loops=False, k=8, symmetric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# cancatenate the same type of trials\n",
    "concat_bold = {}\n",
    "for label in categories:\n",
    "    cur_label_index = y.index[y == label].tolist()\n",
    "    curr_bold_seg = X[cur_label_index]    \n",
    "    concat_bold[label] = curr_bold_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data by time window size and save to file\n",
    "window_length = 1\n",
    "dic_labels = {name: i for i, name in enumerate(categories)}\n",
    "\n",
    "# set output paths\n",
    "split_path = os.path.join(data_dir, 'haxby_split_win/')\n",
    "if not os.path.exists(split_path):\n",
    "    os.makedirs(split_path)\n",
    "out_file = os.path.join(split_path, '{}_{:04d}.npy')\n",
    "out_csv = os.path.join(split_path, 'labels.csv')\n",
    "\n",
    "label_df = pd.DataFrame(columns=['label', 'filename'])\n",
    "for label, ts_data in concat_bold.items():\n",
    "    ts_duration = len(ts_data)\n",
    "    ts_filename = f\"{label}_seg\"\n",
    "    valid_label = dic_labels[label]\n",
    "\n",
    "    # Split the timeseries\n",
    "    rem = ts_duration % window_length\n",
    "    n_splits = int(np.floor(ts_duration / window_length))\n",
    "\n",
    "    ts_data = ts_data[:(ts_duration - rem), :]   \n",
    "\n",
    "    for j, split_ts in enumerate(np.split(ts_data, n_splits)):\n",
    "        ts_output_file_name = out_file.format(ts_filename, j)\n",
    "\n",
    "        split_ts = np.swapaxes(split_ts, 0, 1)\n",
    "        np.save(ts_output_file_name, split_ts)\n",
    "\n",
    "        curr_label = {'label': valid_label, 'filename': os.path.basename(ts_output_file_name)}\n",
    "        label_df = label_df.append(curr_label, ignore_index=True)\n",
    "        \n",
    "label_df.to_csv(out_csv, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "from gcn_windows_dataset import TimeWindowsDataset\n",
    "\n",
    "random_seed = 0\n",
    "\n",
    "train_dataset = TimeWindowsDataset(\n",
    "    data_dir=split_path, \n",
    "    partition=\"train\", \n",
    "    random_seed=random_seed, \n",
    "    pin_memory=True, \n",
    "    normalize=True,\n",
    "    shuffle=True)\n",
    "\n",
    "valid_dataset = TimeWindowsDataset(\n",
    "    data_dir=split_path, \n",
    "    partition=\"valid\", \n",
    "    random_seed=random_seed, \n",
    "    pin_memory=True, \n",
    "    normalize=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_dataset = TimeWindowsDataset(\n",
    "    data_dir=split_path, \n",
    "    partition=\"test\", \n",
    "    random_seed=random_seed, \n",
    "    pin_memory=True, \n",
    "    normalize=True,\n",
    "    shuffle=True)\n",
    "\n",
    "print(\"train dataset: {}\".format(train_dataset))\n",
    "print(\"valid dataset: {}\".format(valid_dataset))\n",
    "print(\"test dataset: {}\".format(test_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a8dfe095fce2b5e88c64a2c3ee084c8e0e0d70b23e7b95b1cfb538be294c5c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
